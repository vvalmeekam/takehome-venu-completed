I have chosen the 3rd task from the assignment to analyze Reddit posts </br></p>
**metrics.py** has code to answer sub-task-1 about defining the attributes of a successful post.  </p>Defining a successful post as that with more engagement (more comment counts) and positive votes (high upvote_ratio and score).  I first looked at the distribution of the score and performed a spearman correlation of the score, upvote_ratio, computed comment counts, tags converted to numeric.  High correlation was observed between score and upvote_ratio.  moderate correlation was observed between score/upvote_ratio and comment counts.  Lastly, built a multivariate linear model to predict score using several attributes of the post.</br></p>  
**mlp_model.py** has code to implement a deep learning model that takes the title and body and predicts the score of the post.  </p>In this model, google bert is used to extract a feature vector for the title and the body of the post.  These vectors are then used as input to a multi-layer perceptron model to learn the score of the corresponding post.  rudimentary clean-up of the input text was performed before tokenizing and extracting feature vectors.  I have used 20% of the data for validation and 80% of the data for training.  Since this is a compute intense task (inspite of using a lightweight bert model), I could not use the entire dataset, but just a small subset.  The model was trained for 150 epochs just to demonstrate an improvement in the loss function (**tt_loss.png** shows a plot of the training and validation loss). </p>Some ideas in terms of improving the model performance include: 1. further refinement of the input text such as removing special characters and numbers. 2. swapping into a full bert model that generates a feature vector of 768 dimensions. 3. perfoming the training with full dataset 4. hyperparameter tuning: optimizing model structure, such as adding drop out layers , optimizing learning rate and number of epochs.
